# GPU Memory Analysis Guide

## ğŸ® GPU ë©”ëª¨ë¦¬ ë¶„ì„ ê¸°ëŠ¥

í•™ìŠµ ì‹œì‘ ì „/ì¤‘ì— GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í•™ìŠµ ì‹œì‘ ì‹œ ìë™ ë¶„ì„

í•™ìŠµì„ ì‹œì‘í•˜ë©´ ìë™ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ë¶„ì„ì´ ì¶œë ¥ë©ë‹ˆë‹¤:

```bash
python scripts/train.py \
    --config configs/default.yaml \
    --data-path /path/to/data.h5
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
======================================================================
ğŸ’¾ Memory Analysis - GPU 0: NVIDIA A100-SXM4-80GB
======================================================================

ğŸ“Š GPU Memory:
  Total:            79.15 GB
  Currently Used:   0.42 GB (0.5%)
  Free:             78.73 GB

ğŸ—ï¸  Model Memory:
  Parameters:       0.0004 GB
  Buffers:          0.0001 GB
  Gradients:        0.0004 GB
  Total Model:      0.0009 GB

ğŸ“¦ Batch Memory (batch_size=128):
  Input Data:       0.0126 GB
  Activations:      0.1260 GB (estimate)
  Batch Gradients:  0.0630 GB (estimate)
  Total Batch:      0.2016 GB (estimate)

ğŸ’¡ Total Estimated Usage:
  Model + Batch:    0.20 GB
  GPU Usage:        0.3%
  Remaining:        78.95 GB

ğŸ¯ Batch Size Recommendations:
  Maximum:          512 (uses ~70% GPU memory)
  Recommended:      307 (balanced)
  Safe:             205 (conservative, very stable)
  Current:          128 âœ… Good

âš™ï¸  Settings:
  Mixed Precision:  Enabled (float16)
  Memory Saved:     ~0.10 GB

======================================================================
```

### 2. ìˆ˜ë™ìœ¼ë¡œ GPU ì •ë³´ í™•ì¸

```bash
# ê¸°ë³¸ GPU ì •ë³´
python scripts/analysis/check_gpu_memory.py

# íŠ¹ì • configë¡œ ë¶„ì„
python scripts/analysis/check_gpu_memory.py --config configs/default.yaml

# íŠ¹ì • ë°°ì¹˜ ì‚¬ì´ì¦ˆ í…ŒìŠ¤íŠ¸
python scripts/analysis/check_gpu_memory.py --batch-size 256

# ìë™ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
python scripts/analysis/check_gpu_memory.py --auto-batch-size
```

### 3. Python ì½”ë“œì—ì„œ ì‚¬ìš©

```python
from utils.gpu_utils import (
    print_gpu_info,
    print_memory_analysis,
    recommend_batch_size,
    auto_select_batch_size
)

# GPU ì •ë³´ ì¶œë ¥
print_gpu_info()

# ë©”ëª¨ë¦¬ ë¶„ì„
print_memory_analysis(model, batch_size=128, mixed_precision=True)

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¶”ì²œ
recommendations = recommend_batch_size(
    model,
    gpu_memory_gb=80.0,
    mixed_precision=True
)
print(f"Recommended batch size: {recommendations['recommended']}")

# ìë™ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
optimal = auto_select_batch_size(model, device)
```

## ğŸ“Š ì œê³µë˜ëŠ” ì •ë³´

### GPU ì •ë³´
- âœ… GPU ê°œìˆ˜
- âœ… GPU ì´ë¦„ (ëª¨ë¸ëª…)
- âœ… ì „ì²´ ë©”ëª¨ë¦¬
- âœ… ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬
- âœ… ë‚¨ì€ ë©”ëª¨ë¦¬
- âœ… Compute capability
- âœ… Multiprocessor ê°œìˆ˜

### ëª¨ë¸ ë©”ëª¨ë¦¬
- âœ… Parameters ë©”ëª¨ë¦¬
- âœ… Buffers ë©”ëª¨ë¦¬
- âœ… Gradients ë©”ëª¨ë¦¬
- âœ… ì´ ëª¨ë¸ ë©”ëª¨ë¦¬

### ë°°ì¹˜ ë©”ëª¨ë¦¬
- âœ… Input ë°ì´í„° ë©”ëª¨ë¦¬
- âœ… Activations ë©”ëª¨ë¦¬ (ì¶”ì •)
- âœ… Batch gradients ë©”ëª¨ë¦¬ (ì¶”ì •)
- âœ… ì´ ë°°ì¹˜ ë©”ëª¨ë¦¬

### ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¶”ì²œ (ëª¨ë‘ 2ì˜ ê±°ë“­ì œê³±)
- âœ… **Maximum**: GPU ë©”ëª¨ë¦¬ì˜ ~70% ì‚¬ìš© (2ì˜ ê±°ë“­ì œê³±)
- âœ… **Recommended**: Maximumì˜ ~60% (ê· í˜•ì¡íŒ, 2ì˜ ê±°ë“­ì œê³±)
- âœ… **Safe**: Maximumì˜ ~40% (ë§¤ìš° ì•ˆì •ì , 2ì˜ ê±°ë“­ì œê³±)
- âœ… **Current**: í˜„ì¬ ì„¤ì • + ìƒíƒœ í‘œì‹œ

**ì™œ 2ì˜ ê±°ë“­ì œê³±?**
- GPU ë©”ëª¨ë¦¬ ì •ë ¬ ìµœì í™”
- ì»¤ë„ ì‹¤í–‰ íš¨ìœ¨ì„± í–¥ìƒ
- ë”¥ëŸ¬ë‹ í‘œì¤€ ê´€í–‰
- ì˜ˆ: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, ...

## ğŸ¯ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„ íƒ ê°€ì´ë“œ

### GPU ë©”ëª¨ë¦¬ë³„ ì¶”ì²œ

#### 80GB GPU (A100)
```yaml
data:
  batch_size: 256  # Recommended
  # Maximum: ~512
  # Safe: ~200
```

#### 40GB GPU (A100 40GB)
```yaml
data:
  batch_size: 128  # Recommended
  # Maximum: ~256
  # Safe: ~100
```

#### 24GB GPU (RTX 3090, 4090)
```yaml
data:
  batch_size: 64   # Recommended
  # Maximum: ~128
  # Safe: ~48
```

#### 16GB GPU (RTX 4060 Ti)
```yaml
data:
  batch_size: 32   # Recommended
  # Maximum: ~64
  # Safe: ~24
```

#### 12GB GPU (RTX 3060)
```yaml
data:
  batch_size: 16   # Recommended
  # Maximum: ~32
  # Safe: ~12
```

### ëª¨ë¸ ì‚¬ì´ì¦ˆë³„ ì¡°ì •

#### Small Model (hidden=64, depth=2)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ë§¤ìš° ì ìŒ
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ìœ„ ì¶”ì²œì˜ 2~3ë°° ê°€ëŠ¥

#### Default Model (hidden=16, depth=3)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì ìŒ
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ìœ„ ì¶”ì²œëŒ€ë¡œ

#### Large Model (hidden=512, depth=8)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ë§ìŒ
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ìœ„ ì¶”ì²œì˜ 1/2~1/4

## ğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™” íŒ

### 1. Mixed Precision ì‚¬ìš© (ê¸°ë³¸ í™œì„±í™”)
```yaml
training:
  use_amp: true  # float16 ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ~50% ì ˆì•½
```

### 2. Gradient Accumulation
```yaml
training:
  batch_size: 64
  gradient_accumulation_steps: 2  # Effective batch size = 128
```

ë©”ëª¨ë¦¬ëŠ” 64ë¡œ ì‚¬ìš©í•˜ì§€ë§Œ íš¨ê³¼ëŠ” 128 ë°°ì¹˜ì™€ ë™ì¼!

### 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„ ì¡°ì ˆ
```yaml
training:
  save_interval: 1000  # ëœ ìì£¼ ì €ì¥ (ë©”ëª¨ë¦¬ peak ê°ì†Œ)
```

### 4. DataLoader Workers
```yaml
data:
  num_workers: 4    # CPU ë³‘ëª© ë°©ì§€
  pin_memory: true  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
```

## ğŸ” ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ì—ëŸ¬ ë°œìƒ ì‹œ

```bash
# 1. í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
python scripts/analysis/check_gpu_memory.py

# 2. ìë™ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
python scripts/analysis/check_gpu_memory.py --auto-batch-size

# 3. ì¶”ì²œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ í•™ìŠµ
python scripts/train.py \
    --config configs/default.yaml \
    --data-path /path/to/data.h5
```

### ìˆ˜ë™ ì¡°ì ˆ

1. **ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°**
   ```yaml
   data:
     batch_size: 64  # 128 â†’ 64
   ```

2. **Mixed Precision í™•ì¸**
   ```yaml
   training:
     use_amp: true  # ë°˜ë“œì‹œ í™œì„±í™”
   ```

3. **Gradient Accumulation ì‚¬ìš©**
   ```yaml
   training:
     gradient_accumulation_steps: 2  # ë©”ëª¨ë¦¬ëŠ” ì ˆë°˜, íš¨ê³¼ëŠ” ë™ì¼
   ```

4. **ëª¨ë¸ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°**
   ```yaml
   model:
     hidden: 16   # 512 â†’ 16
     depth: 3     # 8 â†’ 3
   ```

## ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### Training ì¤‘ ë©”ëª¨ë¦¬ í™•ì¸

```bash
# ìƒˆ í„°ë¯¸ë„ì—ì„œ
watch -n 1 nvidia-smi
```

### Pythonì—ì„œ í™•ì¸

```python
from utils.gpu_utils import monitor_gpu_memory

# í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ
monitor_gpu_memory(device_id=0)

# ë©”ëª¨ë¦¬ ë¦¬ì…‹
monitor_gpu_memory(device_id=0, reset=True)
```

## ğŸ¯ ìµœì  ì„¤ì • ì°¾ê¸°

### Step 1: GPU í™•ì¸
```bash
python scripts/analysis/check_gpu_memory.py
```

### Step 2: ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
```bash
python scripts/analysis/check_gpu_memory.py \
    --config configs/default.yaml \
    --auto-batch-size
```

### Step 3: Config ì—…ë°ì´íŠ¸
```yaml
# configs/default.yaml
data:
  batch_size: <ì¶”ì²œëœ ê°’>
```

### Step 4: í•™ìŠµ ì‹œì‘
```bash
python scripts/train.py --config configs/default.yaml --data-path /path/to/data.h5
```

## ğŸ“ API Reference

### get_gpu_info()
```python
gpu_info = get_gpu_info()
# Returns: List of GPU info dictionaries
```

### print_memory_analysis(model, batch_size, device_id, mixed_precision)
```python
print_memory_analysis(
    model=my_model,
    batch_size=128,
    device_id=0,
    mixed_precision=True
)
# Prints comprehensive memory analysis
```

### recommend_batch_size(model, gpu_memory_gb, safety_margin, mixed_precision)
```python
recommendations = recommend_batch_size(
    model=my_model,
    gpu_memory_gb=80.0,
    safety_margin=0.7,  # Use 70% of GPU
    mixed_precision=True
)
# Returns: {'maximum': 512, 'recommended': 307, 'safe': 205}
```

### auto_select_batch_size(model, device, start_batch)
```python
optimal = auto_select_batch_size(
    model=my_model,
    device=torch.device('cuda:0'),
    start_batch=128
)
# Returns: Largest feasible batch size
```

## âœ¨ ì¥ì 

1. âœ… **ìë™ ë¶„ì„**: í•™ìŠµ ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶„ì„
2. âœ… **ì •í™•í•œ ì¶”ì²œ**: ëª¨ë¸, GPU, ì„¤ì •ì— ë§ì¶˜ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
3. âœ… **OOM ë°©ì§€**: Safe ëª¨ë“œë¡œ ì•ˆì •ì  í•™ìŠµ
4. âœ… **ì„±ëŠ¥ ìµœì í™”**: Maximum ëª¨ë“œë¡œ ìµœëŒ€ í™œìš©
5. âœ… **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ì¶”ì 

Happy Training! ğŸš€

