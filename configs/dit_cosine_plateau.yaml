# DIT + Cosine Noise Schedule + Plateau LR Configuration
# =====================================================
# DIT model with cosine noise schedule and plateau learning rate scheduler

experiment_name: "dit_cosine_plateau"
description: "DIT model with cosine noise schedule and plateau LR scheduler"

# Model configuration (same as default)
model:
  architecture: "dit"
  seq_len: 5160
  hidden: 256
  depth: 4
  heads: 8
  dropout: 0.1
  fusion: "FiLM"
  label_dim: 6
  t_embed_dim: 128
  mlp_ratio: 4.0
  
  # Normalization metadata
  affine_offsets: [0.0, 0.0, -600.0, -550.0, -550.0]
  affine_scales: [100.0, 5.0, 1200.0, 1100.0, 1100.0]
  label_offsets: [0.0, 0.0, 0.0, -600.0, -550.0, -550.0]
  label_scales: [100000000.0, 3.14159, 6.28318, 1200.0, 1100.0, 1100.0]
  time_transform: "ln"

# Diffusion configuration - COSINE SCHEDULE
diffusion:
  timesteps: 1000
  objective: "eps"
  schedule: "cosine"  # Cosine schedule for smoother transitions
  
  # Cosine schedule parameters
  # cosine_s: Small offset to prevent Î²_t from being too small near t=0
  # Lower values (0.001-0.005): More aggressive noise schedule, faster convergence
  # Higher values (0.008-0.02): Smoother transitions, more stable training
  # Default: 0.008 (recommended in DDPM paper)
  cosine_s: 0.008
  
  # Alternative: You can also specify beta_start and beta_end for other schedules
  # beta_start: 1e-4  # Not used in cosine schedule
  # beta_end: 0.02    # Not used in cosine schedule
  
  # Classifier-free guidance
  use_cfg: true
  cfg_scale: 2.0
  cfg_dropout: 0.1

# Data configuration (same as default)
data:
  h5_path: "/home/work/GENESIS/GENESIS-pmj0324/GENESIS/GENESIS-data/22644_0921_time_shift.h5"
  replace_time_inf_with: 0.0
  channel_first: true
  batch_size: 512
  num_workers: 40
  pin_memory: true
  shuffle: true
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  
  # Normalization
  time_transform: "ln"
  affine_offsets: [0.0, 0.0, -600.0, -550.0, -550.0]
  affine_scales: [200.0, 10.0, 1200.0, 1100.0, 1100.0]
  label_offsets: [0.0, 0.0, 0.0, -600.0, -550.0, -550.0]
  label_scales: [100000000.0, 3.14159, 6.28318, 1200.0, 1100.0, 1100.0]

# Training configuration - PLATEAU SCHEDULER
training:
  num_epochs: 250              # More epochs for cosine schedule
  learning_rate: 0.0002        # Slightly higher LR for faster convergence
  weight_decay: 0.01
  grad_clip_norm: 1.0
  optimizer: "AdamW"
  scheduler: "plateau"         # Plateau scheduler
  warmup_steps: 1000
  warmup_ratio: 0.04
  
  # Plateau scheduler parameters
  plateau_patience: 3          # Patience for plateau
  plateau_factor: 0.5
  plateau_mode: "min"
  plateau_threshold: 1e-6
  plateau_cooldown: 0
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 8   # More patience for cosine schedule
  early_stopping_min_delta: 1e-4
  early_stopping_mode: "min"
  early_stopping_baseline: null
  early_stopping_restore_best: true
  early_stopping_verbose: true
  
  # Logging and checkpointing
  log_interval: 50
  save_interval: 1000
  eval_interval: 1
  output_dir: "outputs"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  resume_from_checkpoint: null
  use_amp: true
  debug_mode: false
  detect_anomaly: false

# System settings
device: "auto"
seed: 42

# Logging
use_wandb: false
wandb_project: "icecube-diffusion"
wandb_entity: null


