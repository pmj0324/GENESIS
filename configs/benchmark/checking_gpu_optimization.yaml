# GPU Optimization Benchmark Configuration
# =========================================
# This config is for benchmarking and finding optimal settings
# NOT for actual training!

experiment_name: "gpu_optimization_benchmark"
description: "Benchmark different batch sizes and settings to find optimal configuration"

# Model configuration (same as default for consistency)
model:
  seq_len: 5160
  hidden: 16
  depth: 3
  heads: 8
  dropout: 0.1
  fusion: "SUM"
  label_dim: 6
  t_embed_dim: 128
  mlp_ratio: 4.0
  
  # Normalization
  affine_offsets: [0.0, 0.0, 0.0, 0.0, 0.0]
  affine_scales: [100.0, 10.0, 600.0, 550.0, 550.0]
  label_offsets: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
  label_scales: [50000000.0, 1.0, 1.0, 600.0, 550.0, 550.0]
  
  time_transform: "ln"
  exclude_zero_time: true

# Diffusion configuration
diffusion:
  timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  objective: "eps"
  schedule: "linear"
  use_cfg: true
  cfg_scale: 2.0
  cfg_dropout: 0.1

# Data configuration
data:
  h5_path: "/home/work/GENESIS/GENESIS-data/22644_0921.h5"
  replace_time_inf_with: 0.0
  channel_first: true
  batch_size: 128  # Will be overridden during benchmark
  num_workers: 4   # Will be tested
  pin_memory: true
  shuffle: false   # Disable for consistent benchmarking
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

# Training configuration (minimal for benchmarking)
training:
  num_epochs: 1  # Only 1 epoch for benchmarking
  learning_rate: 0.0001
  weight_decay: 0.01
  grad_clip_norm: 1.0
  optimizer: "AdamW"
  scheduler: null  # No scheduler needed for benchmark
  warmup_steps: 0
  warmup_ratio: 0.0
  
  # Scheduler parameters (not used)
  cosine_t_max: null
  plateau_patience: 10
  plateau_factor: 0.5
  plateau_mode: "min"
  plateau_threshold: 1e-4
  plateau_cooldown: 0
  step_size: 30
  step_gamma: 0.1
  linear_start_factor: 1.0
  linear_end_factor: 0.0
  
  # Early stopping (disabled for benchmark)
  early_stopping: false
  early_stopping_patience: 5
  early_stopping_min_delta: 1e-4
  early_stopping_mode: "min"
  early_stopping_baseline: null
  early_stopping_restore_best: true
  early_stopping_verbose: true
  
  # Logging (minimal for benchmark)
  log_interval: 10
  save_interval: 10000  # Don't save during benchmark
  eval_interval: 10000  # Don't evaluate during benchmark
  
  # Directories
  output_dir: "./benchmark_outputs"
  checkpoint_dir: "./benchmark_checkpoints"
  log_dir: "./benchmark_logs"
  resume_from_checkpoint: null
  
  # Performance
  use_amp: true  # Test with mixed precision
  debug_mode: false
  detect_anomaly: false
  save_best_only: false
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0

# Benchmark-specific settings
benchmark:
  # Batch sizes to test (powers of 2)
  batch_sizes: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
  
  # Number of workers to test
  num_workers_options: [0, 2, 4, 8, 12, 16]
  
  # Number of steps to test per configuration
  steps_per_test: 10  # Test 10 steps per batch size
  
  # Whether to test mixed precision on/off
  test_mixed_precision: true
  
  # Whether to test pin_memory on/off
  test_pin_memory: true

# System settings
device: "auto"
seed: 42

# Logging
use_wandb: false
wandb_project: null
wandb_entity: null

