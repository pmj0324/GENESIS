{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "train_pmt_dit.py\n",
    "\n",
    "- PMTSignalsH5 데이터셋 로딩\n",
    "- PMTDit + GaussianDiffusion 학습\n",
    "- NaN/Inf 디버깅 도구 포함:\n",
    "  * 입력(raw), q_sample(x_t), 모델 출력(eps_hat), loss, grad 순서로 점검\n",
    "  * step 번호와 에폭 진행 퍼센트 출력\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import h5py \n",
    "# === 프로젝트 모듈 import ===\n",
    "from dataloader import make_dataloader\n",
    "from models import PMTDit, GaussianDiffusion, DiffusionConfig\n",
    "from utils import print_h5_structure\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "h5_path = \"/home/work/GENESIS/GENESIS-data/22644_0921.h5\"\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file: /home/work/GENESIS/GENESIS-data/22644_0921.h5\n",
      "[Dataset] info - shape: (178056, 9), dtype: float32\n",
      "[Dataset] input - shape: (178056, 2, 5160), dtype: float32\n",
      "[Dataset] label - shape: (178056, 6), dtype: float32\n",
      "[Dataset] xpmt - shape: (5160,), dtype: float32\n",
      "[Dataset] ypmt - shape: (5160,), dtype: float32\n",
      "[Dataset] zpmt - shape: (5160,), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print_h5_structure(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-256.14 -256.14 -256.14 ...  -10.97  -10.97  -10.97]\n",
      "[-521.08 -521.08 -521.08 ...    6.72    6.72    6.72]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "with h5py.File(h5_path, \"r\") as f:\n",
    "    # geometry (optional)\n",
    "    npe = f[\"input\"][0, 0,:]\n",
    "    time = f[\"input\"][0,0,:]\n",
    "    xpmt = f[\"xpmt\"][:] if \"xpmt\" in f else None\n",
    "    print(xpmt)\n",
    "    ypmt = f[\"ypmt\"][:] if \"ypmt\" in f else None\n",
    "    print(ypmt)\n",
    "    zpmt = f[\"zpmt\"][:] if \"zpmt\" in f else None\n",
    "    #print(zpmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = make_dataloader(\n",
    "    h5_path=h5_path,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    replace_time_inf_with=0.0,   # time의 inf를 0.0으로 치환 (원인 파악 후 조정 가능)\n",
    "    channel_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_finite(name: str, x: torch.Tensor):\n",
    "    \"\"\"NaN/Inf 있으면 통계 찍고 에러 발생\"\"\"\n",
    "    if not torch.isfinite(x).all():\n",
    "        tensor_stats(name, x)\n",
    "        raise RuntimeError(f\"Non-finite values detected in {name}\")\n",
    "    else:\n",
    "        print(f\"[{name}] ✅ all finite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2783\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 0 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([721, 898, 117, 719, 359, 117, 857, 295, 135, 582, 399, 263, 505,  10,\n",
      "        936, 875, 733, 292,  77,  25, 769,  91, 585, 844, 424, 404, 140, 942,\n",
      "        200, 432, 847, 881, 698, 223, 741, 573, 486, 203, 584, 629, 658, 442,\n",
      "        843, 381,  25,  74, 573, 317, 883, 610, 555,   5, 369, 533, 182, 346,\n",
      "        813, 687, 331, 269, 720, 245, 224, 148], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "__________________________________\n",
      "tensor(1.4243, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# DataLoader\n",
    "# ------------------------\n",
    "L = 5160\n",
    "model = PMTDit(\n",
    "    seq_len=L,\n",
    "    hidden=64,\n",
    "    depth=8,\n",
    "    heads=8,\n",
    "    dropout=0.1,\n",
    "    fusion=\"FiLM\",   # or \"SUM\"\n",
    "    label_dim=6,\n",
    "    t_embed_dim=128,\n",
    ").to(device)\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model, DiffusionConfig(timesteps=1000, objective=\"eps\")\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "# 디버깅 옵션(필요할 때 주석 해제)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "# register_nan_hooks(model)\n",
    "\n",
    "# ------------------------\n",
    "# 학습 루프 (NaN 디버깅 포함)\n",
    "# ------------------------\n",
    "steps_per_epoch = len(loader)\n",
    "print(steps_per_epoch)\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step_in_epoch, (x_sig, geom, label, idx) in enumerate(loader, start=1):\n",
    "        # geom shape 맞추기\n",
    "        if geom.ndim == 2:  # (3,L)\n",
    "            geom = geom.unsqueeze(0).expand(x_sig.size(0), -1, -1)\n",
    "\n",
    "        x_sig = x_sig.to(device)    # (B,2,L)\n",
    "        geom  = geom.to(device)     # (B,3,L)\n",
    "        print(geom[0][1])\n",
    "        print(geom[1][1])\n",
    "        label = label.to(device)    # (B,6)\n",
    "\n",
    "        # 1) 입력 원천 점검\n",
    "        try:\n",
    "            print(f\"step in epoch {epoch} : {step_in_epoch}\")\n",
    "            assert_finite(\"x_sig(raw)\", x_sig)\n",
    "            assert_finite(\"geom(raw)\", geom)\n",
    "            assert_finite(\"label(raw)\", label)\n",
    "        except RuntimeError:\n",
    "            print(f\"[BAD INPUT] epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch} idx={idx.tolist()}\")\n",
    "            raise\n",
    "\n",
    "        # 2) q_sample / 모델 출력 점검 (forward만)\n",
    "        with torch.no_grad():\n",
    "            B = x_sig.size(0)\n",
    "            print(f\"batch size of {B}\")\n",
    "            print(f\"diffusion.cfg.timesteps {diffusion.cfg.timesteps}\")\n",
    "            t = torch.randint(0, diffusion.cfg.timesteps, (B,), device=device, dtype=torch.long)\n",
    "            print(f\"time {t}\")\n",
    "            x_sig_t = diffusion.q_sample(x_sig, t)\n",
    "            assert_finite(\"x_sig_t\", x_sig_t)\n",
    "            eps_hat = model(x_sig_t, geom, t, label)\n",
    "            assert_finite(\"eps_hat(fwd-only)\", eps_hat)\n",
    "            print(\"__________________________________\")\n",
    "\n",
    "        # 3) 실제 loss 계산/역전파 (여기서 NaN이면 원인 더 출력)\n",
    "        loss = diffusion.loss(x_sig, geom, label)\n",
    "        print(loss)\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"\\n[WARN] Non-finite loss detected!\")\n",
    "            print(f\"  epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch}, idx={idx.tolist()}\")\n",
    "            tensor_stats(\"loss\", loss)\n",
    "            tensor_stats(\"x_sig(raw)\", x_sig)\n",
    "            tensor_stats(\"geom(raw)\", geom)\n",
    "            tensor_stats(\"label(raw)\", label)\n",
    "            # 필요시 문제 배치 저장\n",
    "            # torch.save({\"x_sig\": x_sig.cpu(), \"geom\": geom.cpu(), \"label\": label.cpu()}, \"bad_batch.pt\")\n",
    "            raise RuntimeError(\"Non-finite loss\")\n",
    "\n",
    "        # 4) 그래디언트 점검\n",
    "        bad_grad = None\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None and not torch.isfinite(p.grad).all():\n",
    "                bad_grad = n\n",
    "                tensor_stats(f\"grad:{n}\", p.grad)\n",
    "                break\n",
    "        if bad_grad is not None:\n",
    "            print(f\"[BAD GRAD] epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch}, param={bad_grad}\")\n",
    "            raise RuntimeError(\"Non-finite gradient\")\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        global_step += 1\n",
    "        # 진행률(퍼센트) 계산\n",
    "        pct = 100.0 * step_in_epoch / steps_per_epoch\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"[epoch {epoch+1}/{num_epochs}] \"\n",
    "                    f\"step {step_in_epoch}/{steps_per_epoch} ({pct:5.1f}%) \"\n",
    "                    f\"| loss = {loss.item():.6f}\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2783\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 0 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([611, 167, 402, 375, 388, 804, 795, 667, 819, 798, 446, 517, 972, 643,\n",
      "        306, 952, 148, 922, 887, 707, 479, 206, 259, 735, 861, 272, 357, 335,\n",
      "        843, 211, 711, 943, 573, 867, 461, 819, 948, 610, 995,  77, 552, 536,\n",
      "        959, 504, 812,  42, 761, 529, 482, 243,  39, 648,  79, 364, 535, 526,\n",
      "        248, 205, 786, 888, 901, 853, 369, 940], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "tensor(2.4717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 1 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([249, 414, 874, 346, 314, 493, 116, 508, 458, 693, 459, 239, 283, 535,\n",
      "        210, 493, 292, 376,  45,  24, 407, 724, 178, 208, 956, 719, 829, 764,\n",
      "        993, 858, 820,  21, 860, 663, 533, 513, 987, 862, 179, 509, 217, 193,\n",
      "        365, 928, 146, 330, 862, 467, 144, 394, 994, 943, 937,  86, 884, 394,\n",
      "        252, 616,  67, 669, 795,  26, 509, 839], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "tensor(1.6513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 2 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([832, 264, 196, 967, 279, 420, 764, 466, 880, 317, 280, 175, 911, 223,\n",
      "        141, 851, 405, 580, 607, 499, 295, 252, 775, 221, 545, 762, 811, 398,\n",
      "        498, 964, 221, 203,  20,  92, 163, 316, 525, 183, 397, 593, 452, 200,\n",
      "        197, 194,  50, 362, 430,  33, 715, 277, 498, 752, 911, 872, 938, 643,\n",
      "        789, 429, 543,  49, 171, 178,  83, 737], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "tensor(1.1095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 3 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([538, 839, 353, 213, 666, 566, 488, 592, 271, 363,  16, 387, 929, 773,\n",
      "        264, 171,  98, 677, 531, 916,   6, 124, 341, 454, 492, 987, 653, 317,\n",
      "        715, 286, 464, 302, 159, 118,  24, 991, 246, 572, 678, 218, 293, 864,\n",
      "        933, 685, 551, 248, 545, 470, 973, 159, 233, 313, 322, 273, 485, 859,\n",
      "        877, 668, 718, 872, 743, 945, 656, 470], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "tensor(0.8466, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "tensor([-521.0800, -521.0800, -521.0800,  ...,    6.7200,    6.7200,\n",
      "           6.7200], device='cuda:0')\n",
      "step in epoch 4 : 1\n",
      "[x_sig(raw)] ✅ all finite\n",
      "[geom(raw)] ✅ all finite\n",
      "[label(raw)] ✅ all finite\n",
      "batch size of 64\n",
      "diffusion.cfg.timesteps 1000\n",
      "time tensor([167, 805, 434, 596,   2, 433, 814, 351,  68, 961, 195, 134, 988,   4,\n",
      "        178,   6, 558, 940, 817, 579,  53, 820, 391, 267, 113, 427, 902, 759,\n",
      "        279, 177, 401, 815,   2, 760, 480, 811, 662, 722, 616,  11, 205, 967,\n",
      "          5, 581,  80, 639, 161, 418,  60,  54, 459, 778, 223, 301, 741, 513,\n",
      "        730, 928, 966, 997, 320, 724, 849, 126], device='cuda:0')\n",
      "[x_sig_t] ✅ all finite\n",
      "[eps_hat(fwd-only)] ✅ all finite\n",
      "tensor(0.7661, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 학습 루프 (NaN 디버깅 포함)\n",
    "# ------------------------\n",
    "steps_per_epoch = len(loader)\n",
    "print(steps_per_epoch)\n",
    "global_step = 0\n",
    "\n",
    "model.train()  # 학습 모드\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step_in_epoch, (x_sig, geom, label, idx) in enumerate(loader, start=1):\n",
    "        # geom shape 맞추기\n",
    "        if geom.ndim == 2:  # (3,L)\n",
    "            geom = geom.unsqueeze(0).expand(x_sig.size(0), -1, -1)\n",
    "\n",
    "        x_sig = x_sig.to(device)    # (B,2,L)\n",
    "        geom  = geom.to(device)     # (B,3,L)\n",
    "        print(geom[0][1])\n",
    "        print(geom[1][1])\n",
    "        label = label.to(device)    # (B,6)\n",
    "\n",
    "        # 1) 입력 원천 점검\n",
    "        try:\n",
    "            print(f\"step in epoch {epoch} : {step_in_epoch}\")\n",
    "            assert_finite(\"x_sig(raw)\", x_sig)\n",
    "            assert_finite(\"geom(raw)\", geom)\n",
    "            assert_finite(\"label(raw)\", label)\n",
    "        except RuntimeError:\n",
    "            print(f\"[BAD INPUT] epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch} idx={idx.tolist()}\")\n",
    "            raise\n",
    "\n",
    "        # 2) q_sample / 모델 출력 점검 (forward만)\n",
    "        with torch.no_grad():\n",
    "            B = x_sig.size(0)\n",
    "            print(f\"batch size of {B}\")\n",
    "            print(f\"diffusion.cfg.timesteps {diffusion.cfg.timesteps}\")\n",
    "            t = torch.randint(0, diffusion.cfg.timesteps, (B,), device=device, dtype=torch.long)\n",
    "            print(f\"time {t}\")\n",
    "            x_sig_t = diffusion.q_sample(x_sig, t)\n",
    "            assert_finite(\"x_sig_t\", x_sig_t)\n",
    "            eps_hat = model(x_sig_t, geom, t, label)\n",
    "            assert_finite(\"eps_hat(fwd-only)\", eps_hat)\n",
    "\n",
    "        # 3) 실제 loss 계산/역전파 (여기서 NaN이면 원인 더 출력)\n",
    "        loss = diffusion.loss(x_sig, geom, label)\n",
    "        print(loss)\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"\\n[WARN] Non-finite loss detected!\")\n",
    "            print(f\"  epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch}, idx={idx.tolist()}\")\n",
    "            tensor_stats(\"loss\", loss)\n",
    "            tensor_stats(\"x_sig(raw)\", x_sig)\n",
    "            tensor_stats(\"geom(raw)\", geom)\n",
    "            tensor_stats(\"label(raw)\", label)\n",
    "            raise RuntimeError(\"Non-finite loss\")\n",
    "\n",
    "        # ====== 파라미터 업데이트 필수 단계 추가 ======\n",
    "        optimizer.zero_grad(set_to_none=True)   # 누적된 grad 초기화\n",
    "        # (옵션) 업데이트 전 파라미터 노름 합 체크\n",
    "        # with torch.no_grad():\n",
    "        #     w_before = sum(p.norm().item() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        loss.backward()                         # 그래디언트 계산\n",
    "\n",
    "        # 4) 그래디언트 점검 (backward 이후)\n",
    "        bad_grad = None\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.grad is not None and not torch.isfinite(p.grad).all():\n",
    "                bad_grad = n\n",
    "                tensor_stats(f\"grad:{n}\", p.grad)\n",
    "                break\n",
    "        if bad_grad is not None:\n",
    "            print(f\"[BAD GRAD] epoch={epoch+1}, step={step_in_epoch}/{steps_per_epoch}, param={bad_grad}\")\n",
    "            raise RuntimeError(\"Non-finite gradient\")\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()                        # ← 실제 파라미터 업데이트\n",
    "\n",
    "        # (옵션) 업데이트 후 파라미터 노름 합 비교\n",
    "        # with torch.no_grad():\n",
    "        #     w_after = sum(p.norm().item() for p in model.parameters() if p.requires_grad)\n",
    "        # print(f\"param norm sum: before={w_before:.6f} -> after={w_after:.6f}\")\n",
    "\n",
    "        global_step += 1\n",
    "        # 진행률(퍼센트) 계산\n",
    "        pct = 100.0 * step_in_epoch / steps_per_epoch\n",
    "        if global_step % 50 == 0:\n",
    "            print(f\"[epoch {epoch+1}/{num_epochs}] \"\n",
    "                  f\"step {step_in_epoch}/{steps_per_epoch} ({pct:5.1f}%) \"\n",
    "                  f\"| loss = {loss.item():.6f}\")\n",
    "\n",
    "        break  # ← 디버깅용: 첫 배치만 돌고 종료. 실제 학습 시 제거!     # ← 디버깅용: 첫 에폭만. 실제 학습 시 제거!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
