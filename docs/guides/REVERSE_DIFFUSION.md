# Reverse Diffusion Process

**Complete Step-by-Step Guide to Sampling**

---

## ðŸŽ¯ Overview

This document explains **exactly** what happens during reverse diffusion sampling, step by step, with emphasis on:
- Where normalization happens
- When to denormalize
- How CFG (Classifier-Free Guidance) works
- Time dimension reduction

---

## ðŸ“Š Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 0: Preparation                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Load checkpoint â†’ get normalization params                   â”‚
â”‚ 2. Prepare conditions (label, geom) in NORMALIZED space         â”‚
â”‚ 3. Check time_transform type: "ln" or "log10"                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Initialize from Gaussian Noise                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ x_T ~ N(0, I)   # Pure Gaussian noise in NORMALIZED space      â”‚
â”‚                 # Shape: (B, 2, 5160)                           â”‚
â”‚                 # Note: Already in normalized space!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Reverse Diffusion Loop (T â†’ 0)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ for t = T-1, T-2, ..., 1, 0: (T-1 is final timestep)          â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ 2.1: Predict Noise with CFG                              â”‚ â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚   â”‚ # Conditional prediction                                 â”‚ â”‚
â”‚   â”‚ eps_cond = model(x_t, geom, t, label)                   â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚ # Unconditional prediction (label = 0)                   â”‚ â”‚
â”‚   â”‚ eps_uncond = model(x_t, geom, t, label_zero)            â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚ # Combine with guidance scale                            â”‚ â”‚
â”‚   â”‚ eps_hat = eps_uncond + cfg_scale * (eps_cond - eps_uncond)â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚ âš ï¸ IMPORTANT: All in NORMALIZED space!                  â”‚ â”‚
â”‚   â”‚ âš ï¸ NO denormalization here!                              â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†“                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ 2.2: DDPM Update Step                                    â”‚ â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚   â”‚ # Compute mean                                            â”‚ â”‚
â”‚   â”‚ mean = (1/âˆšÎ±_t) * (x_t - (Î²_t/âˆš(1-á¾±_t)) * eps_hat)     â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚ # Add noise (except at t=0)                              â”‚ â”‚
â”‚   â”‚ if t > 0:                                                â”‚ â”‚
â”‚   â”‚     noise = randn_like(x_t)                              â”‚ â”‚
â”‚   â”‚     x_{t-1} = mean + âˆš(posterior_variance) * noise      â”‚ â”‚
â”‚   â”‚ else:                                                    â”‚ â”‚
â”‚   â”‚     x_0 = mean  # Final denoised sample                 â”‚ â”‚
â”‚   â”‚                                                           â”‚ â”‚
â”‚   â”‚ âš ï¸ IMPORTANT: Still in NORMALIZED space!                â”‚ â”‚
â”‚   â”‚ âš ï¸ Use x_{t-1} for NEXT iteration!                      â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†“                                   â”‚
â”‚   # Loop continues with x_{t-1}...                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Denormalization (ONLY at the END!)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Now we have x_0 in NORMALIZED space.                            â”‚
â”‚ Time to convert back to physical units:                         â”‚
â”‚                                                                  â”‚
â”‚ 3.1: Affine Inverse                                             â”‚
â”‚      x_physical = (x_0 * scale) + offset                        â”‚
â”‚                                                                  â”‚
â”‚ 3.2: Time Transform Inverse                                     â”‚
â”‚      if time_transform == "ln":                                 â”‚
â”‚          time_raw = exp(time_physical) - 1                      â”‚
â”‚      elif time_transform == "log10":                            â”‚
â”‚          time_raw = 10^(time_physical) - 1                      â”‚
â”‚                                                                  â”‚
â”‚ 3.3: Clamp to prevent overflow                                  â”‚
â”‚      time_raw = clamp(time_raw, min=0, max=1e8)                â”‚
â”‚                                                                  â”‚
â”‚ âœ… Result: Physical units (NPE, ns)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” Detailed Breakdown

### Phase 1: Preparation

```python
# Load model and get metadata
checkpoint = torch.load("checkpoint.pth")
model.load_state_dict(checkpoint['model_state_dict'])

# Get normalization parameters (stored as metadata)
norm_params = model.get_normalization_params()
# {
#   'affine_offsets': [0, 0, -600, -550, -550],
#   'affine_scales': [200, 10, 1200, 1100, 1100],
#   'time_transform': 'ln',  # â† Important!
#   ...
# }

# Prepare conditions (already normalized by dataloader)
label_norm = (label_raw - label_offsets) / label_scales
geom_norm = (geom_raw - geom_offsets[2:5]) / geom_scales[2:5]
```

**Key Points:**
- âœ… Normalization params are **metadata** in model
- âœ… Conditions must be **pre-normalized**
- âœ… `time_transform` tells us how to invert: ln or log10

---

### Phase 2: Gaussian Initialization

```python
# Start from pure noise in normalized space
x_T = torch.randn(B, 2, 5160)  # N(0, 1)
```

**Key Points:**
- âœ… Gaussian noise in **normalized space**
- âœ… This matches the training distribution
- âŒ NOT in physical units yet!

---

### Phase 3: Reverse Loop (Most Important!)

#### Iteration Structure

```python
for t in reversed(range(T)):  # T-1, T-2, ..., 1, 0 (T-1 is final timestep)
    t_batch = torch.full((B,), t, dtype=torch.long)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: Predict noise with CFG
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Conditional prediction
    eps_cond = model(x_t, geom_norm, t_batch, label_norm)
    
    # Unconditional prediction
    label_uncond = torch.zeros_like(label_norm)
    eps_uncond = model(x_t, geom_norm, t_batch, label_uncond)
    
    # Combine with guidance
    eps_hat = eps_uncond + cfg_scale * (eps_cond - eps_uncond)
    
    # âš ï¸ CRITICAL: All operations in NORMALIZED space!
    # âš ï¸ NO denormalization here!
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: DDPM update
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Get schedule parameters
    alpha = alphas[t]
    alpha_bar = alphas_cumprod[t]
    beta = betas[t]
    
    # Compute denoised mean
    # Formula: Î¼_Î¸(x_t,t) = (1/âˆšÎ±_t) * (x_t - (Î²_t/âˆš(1-á¾±_t)) * Îµ_Î¸(x_t,t))
    mean = (1 / sqrt(alpha)) * (
        x_t - (beta / sqrt(1 - alpha_bar)) * eps_hat
    )
    
    # Add noise for next step (except at t=0)
    if t > 0:
        noise = torch.randn_like(x_t)
        variance = sqrt(posterior_variance[t])
        x_t = mean + variance * noise  # â† This becomes x_{t-1}
    else:
        x_t = mean  # Final x_0
    
    # âš ï¸ CRITICAL: x_t (or x_{t-1}) is used for NEXT iteration!
    # âš ï¸ Still in NORMALIZED space!
```

**Key Points:**

1. **Model operates in normalized space**
   - Input: `x_t` (normalized)
   - Output: `eps_hat` (noise prediction, normalized)
   - No denormalization during loop!

2. **Each step produces next input**
   - `x_t` â†’ model â†’ `eps_hat` â†’ update â†’ `x_{t-1}`
   - `x_{t-1}` becomes `x_t` for next iteration
   - Chain continues until `t=0`

3. **CFG combines two predictions**
   - Conditional: Uses actual labels
   - Unconditional: Uses zero labels
   - Guidance scale controls influence

4. **DDPM formula applies**
   - Standard diffusion math
   - All in normalized space
   - Noise added except at final step

---

### Phase 4: Denormalization (ONLY at the end!)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# After loop completes: x_0 in NORMALIZED space
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Step 1: Affine inverse
# x_phys = (x_norm * scale) + offset
charge_phys = (x_0[:, 0, :] * scale[0]) + offset[0]
time_phys = (x_0[:, 1, :] * scale[1]) + offset[1]

# Step 2: Time transform inverse
if time_transform == "ln":
    # Inverse of ln(1+x) is exp(x) - 1
    time_raw = torch.exp(time_phys) - 1
elif time_transform == "log10":
    # Inverse of log10(1+x) is 10^x - 1
    time_raw = torch.pow(10, time_phys) - 1

# Step 3: Clamp to prevent overflow
time_raw = torch.clamp(time_raw, min=0.0, max=1e8)

# Final result
x_physical = torch.stack([charge_phys, time_raw], dim=1)
```

**Key Points:**

1. **Denormalization ONLY at the end**
   - âŒ NOT in the loop!
   - âŒ NOT per iteration!
   - âœ… ONLY after final x_0

2. **Two-stage denormalization**
   - First: Affine inverse (scale, offset)
   - Second: Time transform inverse (exp or pow)
   - Order matters!

3. **Time transform is crucial**
   - Must match what was used in dataloader
   - Stored as metadata in model
   - Retrieved via `get_normalization_params()`

4. **Clamping prevents overflow**
   - `exp()` can produce huge values
   - Clamp to reasonable physical range
   - Prevents inf/nan

---

## âš ï¸ Common Mistakes

### âŒ Mistake 1: Denormalize in Loop

```python
# WRONG!
for t in reversed(range(T)):
    eps_hat = model(x_t, ...)
    x_t = update(x_t, eps_hat)
    x_t = denormalize(x_t)  # âŒ NO! Breaks the chain!
```

**Why wrong?** 
- Model expects normalized input
- Next iteration gets wrong input
- Breaks the diffusion process

### âŒ Mistake 2: Forget Time Transform

```python
# WRONG!
x_physical = (x_0 * scale) + offset  # Only affine inverse
# âŒ Forgot to inverse ln/log10!
```

**Why wrong?**
- Time was transformed with ln(1+x) in dataloader
- Must reverse it: exp(x) - 1
- Without this, time values are wrong

### âŒ Mistake 3: Wrong Time Transform

```python
# WRONG!
time_raw = torch.exp(time_phys)  # âŒ Should be exp(x) - 1
```

**Why wrong?**
- Dataloader used: `ln(1 + time_raw)`
- Inverse is: `exp(time_norm) - 1`, not `exp(time_norm)`
- Off by 1 error!

### âŒ Mistake 4: Use Wrong x_t

```python
# WRONG!
for t in reversed(range(T)):
    eps_hat = model(x_t, ...)
    x_new = update(x_t, eps_hat)
    # x_t stays the same  âŒ NO!
```

**Why wrong?**
- Must use updated `x_new` as next `x_t`
- Each iteration builds on previous
- Chain must be continuous

---

## âœ… Correct Implementation

```python
def sample_correctly(model, label, geom, shape):
    """
    Correct reverse diffusion sampling with proper denormalization.
    """
    B, C, L = shape
    device = model.device
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 1: Get normalization metadata
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    norm_params = model.get_normalization_params()
    offsets = torch.tensor(norm_params['affine_offsets'][:2]).view(2, 1)
    scales = torch.tensor(norm_params['affine_scales'][:2]).view(2, 1)
    time_transform = norm_params['time_transform']
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 2: Initialize from Gaussian
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = torch.randn(B, C, L, device=device)  # Normalized space
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 3: Reverse diffusion loop
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for t_idx in reversed(range(T)):
        t_batch = torch.full((B,), t_idx, device=device, dtype=torch.long)
        
        # CFG: Two forward passes
        eps_cond = model(x, geom, t_batch, label)
        eps_uncond = model(x, geom, t_batch, torch.zeros_like(label))
        eps_hat = eps_uncond + cfg_scale * (eps_cond - eps_uncond)
        
        # DDPM update (still in normalized space!)
        alpha = alphas[t_idx]
        alpha_bar = alphas_cumprod[t_idx]
        beta = betas[t_idx]
        
        mean = (1 / sqrt(alpha)) * (
            x - (beta / sqrt(1 - alpha_bar)) * eps_hat
        )
        
        if t_idx > 0:
            noise = torch.randn_like(x)
            var = sqrt(posterior_variance[t_idx])
            x = mean + var * noise  # â† Update for next iteration
        else:
            x = mean  # Final x_0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Phase 4: Denormalization (ONLY NOW!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Affine inverse
    x = (x * scales) + offsets
    
    # Time transform inverse
    if time_transform == "ln":
        x[:, 1, :] = torch.exp(x[:, 1, :]) - 1
    elif time_transform == "log10":
        x[:, 1, :] = torch.pow(10, x[:, 1, :]) - 1
    
    # Clamp
    x[:, 1, :] = torch.clamp(x[:, 1, :], min=0.0, max=1e8)
    
    return x  # Physical units
```

---

## ðŸ“ Mathematical Details

### Forward Diffusion (Training)

```
q(x_t | x_0) = N(x_t; âˆšá¾±_t x_0, (1 - á¾±_t)I)

x_t = âˆšá¾±_t x_0 + âˆš(1 - á¾±_t) Îµ
```

Where:
- `x_0`: Clean signal (normalized)
- `x_t`: Noisy signal at step t (normalized)
- `Îµ ~ N(0, I)`: Gaussian noise
- `á¾±_t = âˆ_{i=1}^t Î±_i`: Cumulative product

### Reverse Diffusion (Sampling)

```
p_Î¸(x_{t-1} | x_t, c) = N(x_{t-1}; Î¼_Î¸(x_t, t, c), Î£_Î¸(x_t, t))

Î¼_Î¸(x_t, t, c) = (1/âˆšÎ±_t) * (x_t - (Î²_t/âˆš(1-á¾±_t)) * Îµ_Î¸(x_t, t, c))
```

Where:
- `Îµ_Î¸`: Predicted noise from model
- `c`: Condition (label, geometry)
- `Î¼_Î¸`: Mean of reverse distribution
- `Î£_Î¸`: Variance (fixed in DDPM)

### Classifier-Free Guidance

```
Îµ_guided = Îµ_uncond + w * (Îµ_cond - Îµ_uncond)
        = (1 - w) * Îµ_uncond + w * Îµ_cond
```

Where:
- `w`: Guidance scale (cfg_scale)
- `w = 1`: Standard conditional
- `w > 1`: Stronger conditioning
- `w = 0`: Fully unconditional

---

## ðŸŽ“ Key Takeaways

1. **All diffusion happens in normalized space**
   - Gaussian â†’ reverse â†’ x_0 (all normalized)
   - Model never sees physical units during sampling

2. **Denormalize ONLY at the end**
   - After complete reverse diffusion
   - Not per iteration!
   - Two stages: affine + time transform

3. **Time transform is crucial**
   - Must match training (ln or log10)
   - Stored as metadata in model
   - Applied in correct order

4. **Each iteration builds on previous**
   - x_t â†’ model â†’ update â†’ x_{t-1}
   - x_{t-1} is input for next iteration
   - Continuous chain from T to 0

5. **CFG enhances quality**
   - Two forward passes per step
   - Guidance scale controls strength
   - Still in normalized space

---

## ðŸ“š See Also

- `docs/guides/WHY_METADATA.md` - Why metadata exists
- `docs/architecture/NORMALIZATION.md` - Normalization details
- `docs/guides/SAMPLING.md` - Sampling guide
- `diffusion/gaussian_diffusion.py` - Implementation

---

**Last Updated**: 2025-10-11

