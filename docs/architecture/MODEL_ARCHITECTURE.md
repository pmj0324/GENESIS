# Model Architecture

**GENESIS IceCube Diffusion Model**

---

## ğŸ“‹ Overview

GENESIS uses a **Diffusion Transformer (DiT)** architecture for generating IceCube PMT signals conditioned on neutrino event properties.

---

## ğŸ—ï¸ Architecture Components

### 1. Input Structure

```
Input Dimensions:
- x_sig: (B, 2, 5160)  # Signals: [charge, time]
- geom:  (B, 3, 5160)  # Geometry: [x, y, z]
- label: (B, 6)        # Event labels: [Energy, Zenith, Azimuth, X, Y, Z]
- t:     (B,)          # Diffusion timestep

Output:
- eps:   (B, 2, 5160)  # Predicted noise for signals
```

### 2. DiT Model (`PMTDit`)

#### Architecture Flow

```
                                  Input
                                    â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Token Embedder       â”‚
                        â”‚  - Signal MLP         â”‚
                        â”‚  - Geometry MLP       â”‚
                        â”‚  - Fusion (SUM/FiLM)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Positional Embedding â”‚
                        â”‚  (Learned, 5160 tokens)â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Condition Encoders   â”‚
                        â”‚  - Timestep MLP       â”‚
                        â”‚  - Label MLP          â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  DiT Blocks (Ã—depth)  â”‚
                        â”‚  - Self-Attention     â”‚
                        â”‚  - AdaLN (conditional)â”‚
                        â”‚  - FFN                â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Output Projection    â”‚
                        â”‚  (2 channels: charge, â”‚
                        â”‚   time noise)         â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                              Noise Prediction
```

#### Key Features

**Token Embedding:**
- Separate MLPs for signal (charge, time) and geometry (x, y, z)
- Fusion strategies:
  - **SUM**: Simple addition (default, stable)
  - **FiLM**: Feature-wise Linear Modulation (more expressive)

**Positional Embedding:**
- Learned embedding for 5160 PMT positions
- Added to token representations

**Conditioning:**
- **Timestep**: Sinusoidal embedding â†’ MLP
- **Event labels**: Direct MLP encoding
- Both concatenated and fed to DiT blocks via AdaLN

**DiT Blocks:**
- Self-attention for capturing PMT interactions
- Adaptive Layer Normalization (AdaLN) for conditioning
- Feed-forward network with expansion ratio

---

## ğŸ”„ Normalization System

### Design Philosophy

> **Normalization happens in the Dataloader, not in the Model.**

This ensures:
- âœ… Normalization occurs **once per sample** (at loading time)
- âœ… Model forward pass is **faster** (no normalization overhead)
- âœ… **Clear separation of concerns**: Dataloader = preprocessing, Model = learning

### Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. HDF5 Data Loading                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Raw signals:    [charge_raw, time_raw]                    â”‚
â”‚   Raw geometry:   [x_raw, y_raw, z_raw]                     â”‚
â”‚   Raw labels:     [Energy, Zenith, Azimuth, X, Y, Z]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Time Transformation (in Dataloader)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   time_transformed = ln(1 + time_raw)  or  log10(1 + time)  â”‚
â”‚                                                              â”‚
â”‚   Why log(1+x)?                                              â”‚
â”‚   - Handles zeros naturally: ln(1+0) = 0                    â”‚
â”‚   - No special case needed                                  â”‚
â”‚   - Compresses large dynamic range                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Affine Normalization (in Dataloader)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Formula: x_normalized = (x - offset) / scale              â”‚
â”‚                                                              â”‚
â”‚   Applied to:                                                â”‚
â”‚   - Signals:  [charge, time_transformed] â†’ normalized       â”‚
â”‚   - Geometry: [x, y, z] â†’ normalized                        â”‚
â”‚   - Labels:   [Energy, Zenith, ...] â†’ normalized            â”‚
â”‚                                                              â”‚
â”‚   Typical scales:                                            â”‚
â”‚   - charge:  scale=100   (compress NPE values)              â”‚
â”‚   - time:    scale=10    (after ln transform)               â”‚
â”‚   - x,y,z:   scale=600/550 (normalize detector coords)      â”‚
â”‚   - Energy:  scale=5e7   (normalize energy range)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Model Forward (NO normalization here!)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Input:  Already normalized data                           â”‚
â”‚   Output: Predicted noise (also normalized)                 â”‚
â”‚                                                              â”‚
â”‚   Model stores normalization params as METADATA only:       â”‚
â”‚   - affine_offsets, affine_scales                           â”‚
â”‚   - label_offsets, label_scales                             â”‚
â”‚   - time_transform                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Denormalization (for sampling/visualization)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Reverse affine: x = (x_normalized * scale) + offset       â”‚
â”‚   Reverse time:   time_raw = exp(time_normalized) - 1       â”‚
â”‚                   or 10^(time_normalized) - 1                â”‚
â”‚                                                              â”‚
â”‚   Get params: model.get_normalization_params()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Normalization Parameters

**Stored in Model as Buffers (metadata only):**

```python
# Signals + Geometry: [charge, time, x, y, z]
affine_offsets: [0.0, 0.0, 0.0, 0.0, 0.0]
affine_scales:  [100.0, 10.0, 600.0, 550.0, 550.0]

# Labels: [Energy, Zenith, Azimuth, X, Y, Z]
label_offsets: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
label_scales:  [50000000.0, 1.0, 1.0, 600.0, 550.0, 550.0]

# Time transformation method
time_transform: "ln"  # or "log10"
```

**Accessing Metadata:**

```python
norm_params = model.get_normalization_params()
# Returns dict with all normalization information
```

### Why This Design?

| Aspect | Benefit |
|--------|---------|
| **Performance** | âš¡ Forward pass is faster (no normalization overhead) |
| **Clarity** | ğŸ¯ Clear separation: Dataloader=preprocessing, Model=learning |
| **Simplicity** | âœ¨ Metadata-only storage, no version management needed |
| **Consistency** | âœ… Same normalization for train/val/test |

---

## ğŸ›ï¸ Model Configuration

### Default Configuration

```yaml
model:
  seq_len: 5160        # Number of PMTs
  hidden: 512          # Hidden dimension
  depth: 8             # Number of transformer blocks
  heads: 8             # Attention heads
  dropout: 0.1         # Dropout rate
  fusion: "SUM"        # Token fusion strategy
  label_dim: 6         # Event condition dimension
  t_embed_dim: 128     # Timestep embedding dimension
  mlp_ratio: 4.0       # FFN expansion ratio
  
  # Normalization metadata (for denormalization)
  affine_offsets: [0.0, 0.0, 0.0, 0.0, 0.0]
  affine_scales: [100.0, 10.0, 600.0, 550.0, 550.0]
  label_offsets: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
  label_scales: [50000000.0, 1.0, 1.0, 600.0, 550.0, 550.0]
  time_transform: "ln"
```

### Model Sizes

| Size | Hidden | Depth | Heads | Parameters |
|------|--------|-------|-------|------------|
| **Small** | 64 | 2 | 4 | ~0.5M |
| **Medium** | 256 | 4 | 8 | ~8M |
| **Large** | 512 | 8 | 8 | ~50M |
| **XLarge** | 1024 | 12 | 16 | ~200M |

---

## ğŸ”€ Alternative Architectures

GENESIS also supports:

### 1. CNN-based Model (`PMTCNN`)
- Multi-scale convolutions
- For capturing local patterns
- Faster but less expressive

### 2. MLP-based Model (`PMTMLP`)
- Simple fully connected layers
- Baseline for comparison
- Very fast

### 3. Hybrid Model (`HybridModel`)
- CNN for local features
- Transformer for global interactions
- Balance between speed and expressiveness

### 4. ResNet-based Model (`PMTResNet`)
- Residual connections
- Proven architecture
- Good for deep networks

---

## ğŸ“Š Model Factory

Dynamic model creation via configuration:

```python
from models.factory import ModelFactory

model, diffusion = ModelFactory.create_model_and_diffusion(
    model_config,
    diffusion_config
)
```

Automatically:
- âœ… Creates the specified architecture
- âœ… Initializes normalization metadata
- âœ… Sets up diffusion process
- âœ… Moves to correct device

---

## ğŸ¯ Training vs Inference

### Training

```python
# Model receives normalized data from dataloader
x_sig_normalized, geom_normalized, label_normalized = dataloader[i]

# Forward pass (no normalization inside)
noise_pred = model(x_sig_normalized, geom_normalized, t, label_normalized)

# Loss computation on normalized space
loss = mse_loss(noise_pred, true_noise)
```

### Inference/Sampling

```python
# Start from Gaussian noise (normalized space)
x_t = torch.randn(N, 2, 5160)

# Iterative denoising (in normalized space)
for t in reversed(range(T)):
    noise_pred = model(x_t, geom, t, label)
    x_t = diffusion.p_sample(x_t, t, noise_pred)

# Denormalize to get real-world values
norm_params = model.get_normalization_params()
x_real = denormalize_signal(
    x_t,
    norm_params['affine_offsets'],
    norm_params['affine_scales'],
    norm_params['time_transform']
)
```

---

## ğŸ” Implementation Details

### Key Methods

**`PMTDit.__init__(...)`**
- Initializes architecture components
- Registers normalization parameters as buffers

**`PMTDit.forward(x_sig, geom, t, label)`**
- Expects pre-normalized inputs
- Returns noise prediction (normalized)
- NO normalization inside

**`PMTDit.get_normalization_params()`**
- Returns dict with all normalization metadata
- Used for denormalization during sampling

**`PMTDit.set_affine(offsets, scales)`**
- Update normalization metadata
- Does NOT affect forward pass

---

## ğŸ“š Related Documentation

- **Training Guide**: `docs/guides/TRAINING.md`
- **Sampling Guide**: `docs/guides/SAMPLING.md`
- **Normalization Details**: `docs/architecture/NORMALIZATION.md`
- **API Reference**: `docs/reference/API.md`

---

## ğŸ’¡ Key Takeaways

1. **DiT Architecture**: Transformer-based diffusion model for PMT signals
2. **Normalization in Dataloader**: Preprocessing happens once, not per forward pass
3. **Metadata Storage**: Model stores normalization params for denormalization
4. **No Version Management**: Simple metadata is sufficient
5. **Flexible**: Multiple architectures supported via Model Factory

---

**For implementation details, see:** `models/pmt_dit.py`

